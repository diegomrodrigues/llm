{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz54EdZoO3hsD0CHLnR+ug",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a26093e4737045ddb75b0c4ac556b512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69b60ae476164fef88b5ffad8e46391c",
              "IPY_MODEL_ed035935392a446ea0b0b79e4fe5964f",
              "IPY_MODEL_6bad4878bb0144d9879d8b2542f518eb"
            ],
            "layout": "IPY_MODEL_454d2ffb1a8f4758ae3276743bb13daa"
          }
        },
        "69b60ae476164fef88b5ffad8e46391c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1ba5e33a3124b21a00ad3503b08defb",
            "placeholder": "​",
            "style": "IPY_MODEL_66b55c5e81fb404982c269b63354cd62",
            "value": "Batches: 100%"
          }
        },
        "ed035935392a446ea0b0b79e4fe5964f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81cadfdd44e45e3950d2efe7ecd7973",
            "max": 755,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8ec3591db004f4c90f75d0ff164960a",
            "value": 755
          }
        },
        "6bad4878bb0144d9879d8b2542f518eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f24f228212469583e6604ef828493c",
            "placeholder": "​",
            "style": "IPY_MODEL_cac2859cac774502bad089867e4de7a9",
            "value": " 755/755 [01:07&lt;00:00, 10.29it/s]"
          }
        },
        "454d2ffb1a8f4758ae3276743bb13daa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1ba5e33a3124b21a00ad3503b08defb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b55c5e81fb404982c269b63354cd62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f81cadfdd44e45e3950d2efe7ecd7973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8ec3591db004f4c90f75d0ff164960a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10f24f228212469583e6604ef828493c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cac2859cac774502bad089867e4de7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c6fbbd8dec14d2f998c6dc35dbd65a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_861c47b3b5184c65a18a3eaf1e510b47",
              "IPY_MODEL_3231aa12f48c483297d6aa40cc13272d",
              "IPY_MODEL_ab1212db0a3846b99a4a3b10f13c3727"
            ],
            "layout": "IPY_MODEL_d407834798bf49f9a93958536604b950"
          }
        },
        "861c47b3b5184c65a18a3eaf1e510b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_648e83a612e141cba9ba6dc2d263fd76",
            "placeholder": "​",
            "style": "IPY_MODEL_0d9c5527ed614c0b89d7f7ae90135b71",
            "value": "Batches: 100%"
          }
        },
        "3231aa12f48c483297d6aa40cc13272d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be971fdbe0414d37abb4a8d8ec29a42e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18d18f814233451a8109277ac7eac635",
            "value": 1
          }
        },
        "ab1212db0a3846b99a4a3b10f13c3727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c2796e735a04da9b810efb8ba04e890",
            "placeholder": "​",
            "style": "IPY_MODEL_bc0e8c0bb59449bd87bb0e858bacc8f7",
            "value": " 1/1 [00:00&lt;00:00, 17.04it/s]"
          }
        },
        "d407834798bf49f9a93958536604b950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "648e83a612e141cba9ba6dc2d263fd76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d9c5527ed614c0b89d7f7ae90135b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be971fdbe0414d37abb4a8d8ec29a42e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d18f814233451a8109277ac7eac635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c2796e735a04da9b810efb8ba04e890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc0e8c0bb59449bd87bb0e858bacc8f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegomrodrigues/llm/blob/main/RAG_de_Artigos_Arxiv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_experimental langchain_huggingface sentence-transformers pypdf arxiv pymupdf faiss-gpu google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX7Pn0QdR2yd",
        "outputId": "a5c78b22-d6d0-417e-868b-2fd29014a50c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.0.62-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.7/202.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Collecting langchain-community<0.3.0,>=0.2.6 (from langchain_experimental)\n",
            "  Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3.0,>=0.2.10 (from langchain_experimental)\n",
            "  Downloading langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.23.4)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests~=2.32.0 (from arxiv)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.6 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-ai-generativelanguage==0.6.4 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.16.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.7.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai) (1.24.0)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.6 (from langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading langsmith-0.1.83-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (8.4.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain_experimental)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_experimental)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.6->langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=f581a0e14975fa6f7cb3ede544d73d158a648830060a67f65f682801ae16155b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, faiss-gpu, requests, pypdf, PyMuPDFb, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, jsonpointer, feedparser, typing-inspect, pymupdf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jsonpatch, arxiv, nvidia-cusolver-cu12, langsmith, dataclasses-json, langchain-core, sentence-transformers, langchain-text-splitters, langchain_huggingface, langchain, langchain-community, langchain_experimental\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDFb-1.24.6 arxiv-2.1.3 dataclasses-json-0.6.7 faiss-gpu-1.7.2 feedparser-6.0.11 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-community-0.2.6 langchain-core-0.2.10 langchain-text-splitters-0.2.2 langchain_experimental-0.0.62 langchain_huggingface-0.0.3 langsmith-0.1.83 marshmallow-3.21.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 orjson-3.10.5 pymupdf-1.24.7 pypdf-4.2.0 requests-2.32.3 sentence-transformers-3.0.1 sgmllib3k-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93Yqbhr2RI5k",
        "outputId": "4801fa4e-d3d1-4640-f013-05f8c657be4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 56129  100 56129    0     0   230k      0 --:--:-- --:--:-- --:--:--  230k\n"
          ]
        }
      ],
      "source": [
        "!curl -o readme.md https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/README.md"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_links_from_markdown(markdown_content):\n",
        "    # Regular expression to match Markdown links\n",
        "    # This regex matches both [text](url) and bare URL formats\n",
        "    link_pattern = r'\\[([^\\]]+)\\]\\(([^)]+)\\)|(?<!\\()(?:https?://\\S+)'\n",
        "    return re.findall(link_pattern, markdown_content)\n",
        "\n",
        "def scrape_links(markdown_file_path):\n",
        "    # Read the Markdown file\n",
        "    with open(markdown_file_path, 'r', encoding='utf-8') as file:\n",
        "        markdown_content = file.read()\n",
        "\n",
        "    # Extract links from the Markdown content\n",
        "    links = extract_links_from_markdown(markdown_content)\n",
        "\n",
        "    pdf_to_downloads = []\n",
        "\n",
        "    # Process each link\n",
        "    for link in links:\n",
        "        if isinstance(link, tuple):\n",
        "            # This is a [text](url) style link\n",
        "            text, url = link\n",
        "        else:\n",
        "            # This is a bare URL\n",
        "            url = link\n",
        "            text = url\n",
        "\n",
        "        print(f\"Link text: {text}\")\n",
        "        print(f\"URL: {url}\")\n",
        "\n",
        "        if not url.startswith(\"http\") or \"manning\" in url:\n",
        "            continue\n",
        "\n",
        "        if url.endswith(\".pdf\"):\n",
        "            pdf_to_downloads.append(url)\n",
        "\n",
        "        elif \"arxiv\" in url:\n",
        "            parts = url.split(\"/\")\n",
        "            arxiv_id = parts[-1]\n",
        "\n",
        "            pdf_to_downloads.append(f\"https://arxiv.org/pdf/{arxiv_id}.pdf\")\n",
        "\n",
        "        elif \"github\" not in url and not \"img\" in url:\n",
        "            try:\n",
        "                print(f\"Fetching url {url}\")\n",
        "                # Fetch the linked page\n",
        "                response = requests.get(url)\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Extract the title of the linked page\n",
        "                urls = soup.find_all('a') if soup.find('a') else None\n",
        "\n",
        "                if urls:\n",
        "                    for url in urls:\n",
        "                        if url.has_attr('href') and url['href'].endswith('.pdf'):\n",
        "                            pdf_url = url['href']\n",
        "\n",
        "                            if \"arxiv\" in pdf_url:\n",
        "                                print(f\"Found PDF link: {pdf_url}\")\n",
        "                                pdf_to_downloads.append(pdf_url)\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching {url}: {str(e)}\")\n",
        "\n",
        "        print(\"---\")\n",
        "\n",
        "    pdf_to_downloads = list(set(pdf_to_downloads))\n",
        "\n",
        "    print(\"PDF Links:\")\n",
        "    for pdf_link in pdf_to_downloads:\n",
        "        print(pdf_link)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "    return pdf_to_downloads\n",
        "\n"
      ],
      "metadata": {
        "id": "dsDBrxXkRMlO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "from langchain.document_loaders.merge import MergedDataLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "gjcWPFBzRdXD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    show_progress=True,\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")"
      ],
      "metadata": {
        "id": "EC1YmH8FUTYX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_to_download = scrape_links(\"readme.md\")\n",
        "\n",
        "arxiv_ids = []\n",
        "\n",
        "for pdf in pdf_to_download:\n",
        "    if \"arxiv\" in pdf:\n",
        "        parts = pdf.split(\"/\")\n",
        "        arxiv_id = parts[-1][:-4]\n",
        "        arxiv_ids.append(arxiv_id)\n",
        "\n",
        "docs_to_merge = []\n",
        "\n",
        "for arxiv_id in arxiv_ids:\n",
        "    loader = ArxivLoader(query=arxiv_id)\n",
        "    docs_to_merge.append(loader)\n",
        "\n",
        "print(f\"Total documents: {len(docs_to_merge)}\")\n",
        "\n",
        "all_loaders = MergedDataLoader(loaders=docs_to_merge)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=32)\n",
        "\n",
        "all_chunks = all_loaders.load_and_split(text_splitter)\n",
        "\n",
        "print(f\"Total chunks: {len(all_chunks)}\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(all_chunks, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "INqNvH1kR0my",
        "outputId": "491e231a-9d75-4e32-cae3-c986d53bd818"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Link text: ![Awesome\n",
            "URL: https://awesome.re/badge.svg\n",
            "Fetching url https://awesome.re/badge.svg\n",
            "---\n",
            "Link text: LibreChat\n",
            "URL: https://github.com/danny-avila/LibreChat\n",
            "---\n",
            "Link text: Open-Sora\n",
            "URL: https://github.com/hpcaitech/Open-Sora\n",
            "---\n",
            "Link text: LLM101n\n",
            "URL: https://github.com/karpathy/LLM101n\n",
            "---\n",
            "Link text: Gemma 2\n",
            "URL: https://blog.google/technology/developers/google-gemma-2/\n",
            "Fetching url https://blog.google/technology/developers/google-gemma-2/\n",
            "---\n",
            "Link text: Awesome-LLM \n",
            "URL: #awesome-llm-\n",
            "Link text: Milestone Papers\n",
            "URL: #milestone-papers\n",
            "Link text: Other Papers\n",
            "URL: #other-papers\n",
            "Link text: LLM Leaderboard\n",
            "URL: #llm-leaderboard\n",
            "Link text: Open LLM\n",
            "URL: #open-llm\n",
            "Link text: LLM Data\n",
            "URL: #llm-data\n",
            "Link text: LLM Evaluation\n",
            "URL: #llm-evaluation\n",
            "Link text: LLM Training Framework\n",
            "URL: #llm-training-frameworks\n",
            "Link text: LLM Deployment\n",
            "URL: #llm-deployment\n",
            "Link text: LLM Applications\n",
            "URL: #llm-applications\n",
            "Link text: LLM Books\n",
            "URL: #llm-books\n",
            "Link text: Great thoughts about LLM\n",
            "URL: #great-thoughts-about-llm\n",
            "Link text: Miscellaneous\n",
            "URL: #miscellaneous\n",
            "Link text: Attention Is All You Need\n",
            "URL: https://arxiv.org/pdf/1706.03762.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F204e3073870fae3d05bcbc2f6a8e263d9b72e776%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Improving Language Understanding by Generative Pre-Training\n",
            "URL: https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcd18800a0fe0b668a1cc19f2ec95b5003d0a5035%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
            "URL: https://aclanthology.org/N19-1423.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdf2b0e26d0599ce3e70df8a9da02e51594e0e992%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Language Models are Unsupervised Multitask Learners\n",
            "URL: https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9405cc0d6169988371b2755e573cc28650d14dfe%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n",
            "URL: https://arxiv.org/pdf/1909.08053.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8323c591e119eb09b28b29fd6c7bc76bd889df7a%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
            "URL: https://jmlr.org/papers/v21/20-074.html\n",
            "Fetching url https://jmlr.org/papers/v21/20-074.html\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3cfb319689f06bf04c2e28399361f414ca32c4b3%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\n",
            "URL: https://arxiv.org/pdf/1910.02054.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F00c957711b12468cb38424caccdf5291bb354033%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Scaling Laws for Neural Language Models\n",
            "URL: https://arxiv.org/pdf/2001.08361.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe6c561d02500b2596a230b341a8eb8b921ca5bf2%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Language models are few-shot learners\n",
            "URL: https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6b85b63579a916f705a8e10a49bd8d849d91b1fc%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n",
            "URL: https://arxiv.org/pdf/2101.03961.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ffdacf2a732f55befdc410ea927091cad3b791f13%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Evaluating Large Language Models Trained on Code\n",
            "URL: https://arxiv.org/pdf/2107.03374.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Facbdbf49f9bc3f151b93d9ca9a06009f4f6eb269%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: On the Opportunities and Risks of Foundation Models\n",
            "URL: https://arxiv.org/pdf/2108.07258.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4f68e07c6c3173480053fd52391851d6f80d651b%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Finetuned Language Models are Zero-Shot Learners\n",
            "URL: https://openreview.net/forum?id=gEZrGCozdqR\n",
            "Fetching url https://openreview.net/forum?id=gEZrGCozdqR\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fff0b2681d7b05e16c46dfb71d980cc2f605907cd%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Multitask Prompted Training Enables Zero-Shot Task Generalization\n",
            "URL: https://arxiv.org/abs/2110.08207\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F17dd3555fd1ccf1141cf984347fa1b3fd6b009ca%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\n",
            "URL: https://arxiv.org/pdf/2112.06905.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F80d0116d77beeded0c23cf48946d9d10d4faee14%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: WebGPT: Browser-assisted question-answering with human feedback\n",
            "URL: https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22\n",
            "Fetching url https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2f3efe44083af91cef562c1a3451eee2f8601d22%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Improving language models by retrieving from trillions of tokens\n",
            "URL: https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens\n",
            "Fetching url https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F002c256d30d6be4b23d365a8de8ae0e67e4c9641%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher\n",
            "URL: https://arxiv.org/pdf/2112.11446.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F68f141724814839d556a989646194be88641b143%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
            "URL: https://arxiv.org/pdf/2201.11903.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F1b6e810ce0afd0dd093f789d2b2742d047e316d5%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: LaMDA: Language Models for Dialog Applications\n",
            "URL: https://arxiv.org/pdf/2201.08239.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb3848d32f7294ec708627897833c4097eb4d8778%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Solving Quantitative Reasoning Problems with Language Models\n",
            "URL: https://arxiv.org/abs/2206.14858\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fab0e3d3e4d42369de5933a3b4c237780b41c0d77%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\n",
            "URL: https://arxiv.org/pdf/2201.11990.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7cbc2a7843411a1768ab762930707af0a3c33a19%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Training language models to follow instructions with human feedback\n",
            "URL: https://arxiv.org/pdf/2203.02155.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd766bffc357127e0dc86dd69561d5aeb520d6f4c%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: PaLM: Scaling Language Modeling with Pathways\n",
            "URL: https://arxiv.org/pdf/2204.02311.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F094ff971d6a8b8ff870946c9b3ce5aa173617bfb%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: An empirical analysis of compute-optimal large language model training\n",
            "URL: https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training\n",
            "Fetching url https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fbb0656031cb17adf6bac5fd0fe8d53dd9c291508%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: OPT: Open Pre-trained Transformer Language Models\n",
            "URL: https://arxiv.org/pdf/2205.01068.pdf\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F13a0d8bb38f739990c8cd65a44061c6534f17221%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Unifying Language Learning Paradigms\n",
            "URL: https://arxiv.org/abs/2205.05131v1\n",
            "---\n",
            "Link text: Dynamic JSON Badge\n",
            "URL: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff40aeae3e522ada1f6a9f326841b01ef5c8657b6%3Ffields%3DcitationCount&query=%24.citationCount&label=citation\n",
            "---\n",
            "Link text: Emergent Abilities of Large Language Models\n",
            "URL: https://openreview.net/pdf?id=yzkSU5zdwD\n",
            "Fetching url https://openreview.net/pdf?id=yzkSU5zdwD\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-43341daa885b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpdf_to_download\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"readme.md\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marxiv_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdf_to_download\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-58ca3bbeeceb>\u001b[0m in \u001b[0;36mscrape_links\u001b[0;34m(markdown_file_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# Fetch the linked page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m# Extract the title of the linked page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;31m# Fallback to auto-detected encoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapparent_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;31m# Decode unicode from given encoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mapparent_encoding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;34m\"\"\"The apparent encoding, provided by the charset_normalizer or chardet libraries.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchardet\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0;31m# If no character detection library is available, we'll fall back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chardet/__init__.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(byte_str, should_rename_legacy)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mbyte_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUniversalDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshould_rename_legacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshould_rename_legacy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chardet/universaldetector.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMacRomanProber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mprober\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFOUND_IT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m                     self.result = {\n\u001b[1;32m    276\u001b[0m                         \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chardet/charsetgroupprober.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chardet/sbcharsetprober.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m    120\u001b[0m                         \u001b[0mlm_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_order\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seq_counters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlm_cat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mcharset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharset_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.from_documents(all_chunks, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a26093e4737045ddb75b0c4ac556b512",
            "69b60ae476164fef88b5ffad8e46391c",
            "ed035935392a446ea0b0b79e4fe5964f",
            "6bad4878bb0144d9879d8b2542f518eb",
            "454d2ffb1a8f4758ae3276743bb13daa",
            "d1ba5e33a3124b21a00ad3503b08defb",
            "66b55c5e81fb404982c269b63354cd62",
            "f81cadfdd44e45e3950d2efe7ecd7973",
            "b8ec3591db004f4c90f75d0ff164960a",
            "10f24f228212469583e6604ef828493c",
            "cac2859cac774502bad089867e4de7a9"
          ]
        },
        "id": "3Sf1ak6VzuSI",
        "outputId": "a462e0df-8bfd-41d5-8399-8fc6ea226ee0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/755 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a26093e4737045ddb75b0c4ac556b512"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.save_local(\"./arxiv_retriever\")"
      ],
      "metadata": {
        "id": "NkWbWD9R10EN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./arxiv_retriever.zip ./arxiv_retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3owFJYrl2Q65",
        "outputId": "7d99d1ca-5a5e-4d0b-d5aa-3cf705c993de"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: arxiv_retriever/ (stored 0%)\n",
            "  adding: arxiv_retriever/index.faiss (deflated 7%)\n",
            "  adding: arxiv_retriever/index.pkl (deflated 65%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./arxiv_retriever.zip -d ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9pGHgVJR_Dp",
        "outputId": "143ed942-f5ed-46f5-ccf4-817a315acbc5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./arxiv_retriever.zip\n",
            "   creating: ./arxiv_retriever/\n",
            "  inflating: ./arxiv_retriever/index.faiss  \n",
            "  inflating: ./arxiv_retriever/index.pkl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.load_local(\"./arxiv_retriever\", embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "yCMzh7_ESd9J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, retriever):\n",
        "    results = retriever.invoke(query)\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    for idx, doc in enumerate(results):\n",
        "        document = (\n",
        "            f\"<Document index={idx+1} title={doc.metadata['Title']}>\" +\n",
        "                \"<Sumary>\" +\n",
        "                    doc.metadata['Summary'] +\n",
        "                \"</Sumary>\" +\n",
        "                \"<Content>\" +\n",
        "                    doc.page_content +\n",
        "                \"</Content>\" +\n",
        "            \"</Document>\"\n",
        "        )\n",
        "\n",
        "        documents.append(document)\n",
        "\n",
        "    return documents\n",
        "\n",
        "def create_prompt_for_summary(query):\n",
        "    with open(\"Template para RAG Artigos.md\", \"r\") as f:\n",
        "        template = f.read()\n",
        "\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 15})\n",
        "\n",
        "    documents = retrieve(query, retriever)\n",
        "\n",
        "    context = \"\\n\".join(documents)\n",
        "    context = f\"\"\"<context>\n",
        "    {context}\n",
        "    </context>\"\"\"\n",
        "\n",
        "    template = template.replace(\"<context></context>\", context)\n",
        "    template = (\n",
        "        template + \"\\n\\n\" +\n",
        "        \"X = \" + query + \"\\n\\n\" +\n",
        "        \"Resposta em português:\"\n",
        "    )\n",
        "\n",
        "    display(template)\n",
        "\n",
        "X = \"Stability of training Large Language Models\"\n",
        "\n",
        "create_prompt_for_summary(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189,
          "referenced_widgets": [
            "2c6fbbd8dec14d2f998c6dc35dbd65a0",
            "861c47b3b5184c65a18a3eaf1e510b47",
            "3231aa12f48c483297d6aa40cc13272d",
            "ab1212db0a3846b99a4a3b10f13c3727",
            "d407834798bf49f9a93958536604b950",
            "648e83a612e141cba9ba6dc2d263fd76",
            "0d9c5527ed614c0b89d7f7ae90135b71",
            "be971fdbe0414d37abb4a8d8ec29a42e",
            "18d18f814233451a8109277ac7eac635",
            "6c2796e735a04da9b810efb8ba04e890",
            "bc0e8c0bb59449bd87bb0e858bacc8f7"
          ]
        },
        "id": "UFLu7wKGUeZw",
        "outputId": "41e889fb-15ae-45fb-cc10-a8286bbeec32"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c6fbbd8dec14d2f998c6dc35dbd65a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"You are Perplexica, an AI model who is expert at searching the web and answering user's queries.\\n\\nGenerate a response that is informative and relevant to the user's query based on provided context (the context consits of search results containg a brief description of the content of that page).\\nYou must use this context to answer the user's query in the best way possible. Use an unbaised and journalistic tone in your response. Do not repeat the text.\\nYou must not tell the user to open any link or visit any website to get the answer. You must provide the answer in the response itself. If the user asks for links you can provide them.\\nYour responses should be long in length be informative and relevant to the user's query. You can use markdowns to format your response. You should use the template provided below in \\\\`template\\\\` section. Make sure the answer is not short and is informative.\\nYou have to cite the answer using [number] notation. You must cite the sentences with their relevent context number. You must cite each and every part of the answer so the user can know where the information is coming from.\\nPlace these citations at the end of that particular sentence. You can cite the same sentence multiple times if it is relevant to the user's query like [number1][number2].\\nHowever you do not need to cite it using the same number. You can use different numbers to cite the same sentence multiple times. The number refers to the number of the search result (passed in the context) used to generate that part of the answer.\\n\\n\\n\\nCom base nos documentos em <context> retornados por um sistema de buscas em artigos científicos armazenados no ArXiv, use o <template> fornecido abaixo para criar um resumo abrangente que contenha cada um aplicando o template passando o nome do capítulo ou principal conceito explorado nele como o **X = ** do <template>. \\n\\nDiretrizes para o resumo:\\nOs resumos devem ser avançados;\\nOs resumos devem ser baseados nos principais aspectos do conceito abordado no texto, como técnicas ou funcionalidades específicas demonstradas em cada subcapítulo;\\nO resumo deve conter todas principais informações presentes no texto sem omitir nenhum dado importante, com foco especial em não pular nenhum conceitos, resultados importante, argumentos, etc;\\nO resumo deve conter as equações apresentadas, tabelas e outras informações críticas para um entendimento aprofundando e avançado do conteúdo;\\nO resumo deve ser escrito de uma maneira acadêmica, do not repeat text.\\nVocê deve usar o <context> da melhor maneira possível para responder a query do usuário e escrever o resumo segundo as diretrizes;\\nYou must not tell the user to open any link or visit any website to get the answer. You must provide the answer in the response itself;\\nVocê não deve pedir para o usuário abrir um link ou visitar um site para ver a resposta. Você deve responder você mesmo;\\nYou have to cite the answer using [number] notation. The number is the idx on the documents. You must cite the sentences with their relevent context number. You must cite each and every part of the answer so the user can know where the information is coming from.\\nPlace these citations at the end of that particular sentence. You can cite the same sentence multiple times if it is relevant to the user's query like [number1][number2].\\nHowever you do not need to cite it using the same number. You can use different numbers to cite the same sentence multiple times. The number refers to the number of the search result (passed in the context) used to generate that part of the answer.\\nColoque os resultados um texto coerente ao invés de apenas listar em tópicos, também foque em usar as formatações mostradas no template.\\n\\nAything inside the following \\\\`context\\\\` HTML block provided below is for your knowledge returned by the search engine and is not shared by the user. You have to answer question on the basis of it and cite the relevant information from it but you do not have to talk about the context in your response. \\n\\n<context>\\n    <Document index=1 title=Training Compute-Optimal Large Language Models><Sumary>We investigate the optimal model size and number of tokens for training a\\ntransformer language model under a given compute budget. We find that current\\nlarge language models are significantly undertrained, a consequence of the\\nrecent focus on scaling language models whilst keeping the amount of training\\ndata constant. By training over 400 language models ranging from 70 million to\\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\\ncompute-optimal training, the model size and the number of training tokens\\nshould be scaled equally: for every doubling of model size the number of\\ntraining tokens should also be doubled. We test this hypothesis by training a\\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\\nas Gopher but with 70B parameters and 4$\\\\times$ more more data. Chinchilla\\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\\nevaluation tasks. This also means that Chinchilla uses substantially less\\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\\non the MMLU benchmark, greater than a 7% improvement over Gopher.</Sumary><Content>how performance of large language models and toxicity interact is an important future research\\nquestion.\\nWhile we have applied our methodology towards the training of auto-regressive language models,\\nwe expect that there is a similar trade-oﬀbetween model size and the amount of data in other\\nmodalities. As training large models is very expensive, choosing the optimal model size and training\\nsteps beforehand is essential. The methods we propose are easy to reproduce in new settings.\\n6. Acknowledgements\\nWe’d like to thank Jean-baptiste Alayrac, Kareem Ayoub, Chris Dyer, Nando de Freitas, Demis Hassabis,\\nGeoﬀrey Irving, Koray Kavukcuoglu, Nate Kushman and Angeliki Lazaridou for useful comments on\\nthe manuscript. We’d like to thank Andy Brock, Irina Higgins, Michela Paganini, Francis Song, and\\nother colleagues at DeepMind for helpful discussions. We are also very grateful to the JAX and XLA\\nteam for their support and assistance.\\nReferences</Content></Document>\\n<Document index=2 title=Scaling Laws for Neural Language Models><Sumary>We study empirical scaling laws for language model performance on the\\ncross-entropy loss. The loss scales as a power-law with model size, dataset\\nsize, and the amount of compute used for training, with some trends spanning\\nmore than seven orders of magnitude. Other architectural details such as\\nnetwork width or depth have minimal effects within a wide range. Simple\\nequations govern the dependence of overfitting on model/dataset size and the\\ndependence of training speed on model size. These relationships allow us to\\ndetermine the optimal allocation of a fixed compute budget. Larger models are\\nsignificantly more sample-efficient, such that optimally compute-efficient\\ntraining involves training very large models on a relatively modest amount of\\ndata and stopping significantly before convergence.</Sumary><Content>Taken together, these results show that language modeling performance improves smoothly and predictably\\nas we appropriately scale up model size, data, and compute. We expect that larger language models will\\nperform better and be more sample efﬁcient than current models.\\n3\\nLarger models require fewer samples \\nto reach the same performance\\n10\\n8\\n6\\n4\\nThe optimal model size grows smoothly \\nwith the loss target and compute budget\\nLine color indicates \\nnumber of parameters\\n107\\n109\\n1011\\nTokens Processed\\nCompute (PF-days)\\n10-9\\n10-6\\n10-3\\n100\\nTest Loss\\nCompute-eﬃcient \\ntraining stops far \\nshort of convergence\\n103\\n109\\n106\\n103 Params\\n109 Params\\n10\\n8\\n6\\n4\\nFigure 2\\nWe show a series of language model training runs, with models ranging in size from 103 to 109\\nparameters (excluding embeddings).\\n100x Batch Size\\n<10x Serial Steps\\n>1,000,000x Model Size\\nData requirements \\ngrow relatively slowly\\nOptimal model size \\nincreases very quickly\\nMinimum serial steps \\nincreases negligibly\\nFigure 3</Content></Document>\\n<Document index=3 title=Training Compute-Optimal Large Language Models><Sumary>We investigate the optimal model size and number of tokens for training a\\ntransformer language model under a given compute budget. We find that current\\nlarge language models are significantly undertrained, a consequence of the\\nrecent focus on scaling language models whilst keeping the amount of training\\ndata constant. By training over 400 language models ranging from 70 million to\\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\\ncompute-optimal training, the model size and the number of training tokens\\nshould be scaled equally: for every doubling of model size the number of\\ntraining tokens should also be doubled. We test this hypothesis by training a\\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\\nas Gopher but with 70B parameters and 4$\\\\times$ more more data. Chinchilla\\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\\nevaluation tasks. This also means that Chinchilla uses substantially less\\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\\non the MMLU benchmark, greater than a 7% improvement over Gopher.</Sumary><Content>Training Compute-Optimal Large Language Models\\nJordan Hoﬀmann★, Sebastian Borgeaud★, Arthur Mensch★, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\\nErich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre★\\n★Equal contributions\\nWe investigate the optimal model size and number of tokens for training a transformer language model\\nunder a given compute budget. We ﬁnd that current large language models are signiﬁcantly under-\\ntrained, a consequence of the recent focus on scaling language models whilst keeping the amount of\\ntraining data constant. By training over 400 language models ranging from 70 million to over 16 billion\\nparameters on 5 to 500 billion tokens, we ﬁnd that for compute-optimal training, the model size and</Content></Document>\\n<Document index=4 title=Training Compute-Optimal Large Language Models><Sumary>We investigate the optimal model size and number of tokens for training a\\ntransformer language model under a given compute budget. We find that current\\nlarge language models are significantly undertrained, a consequence of the\\nrecent focus on scaling language models whilst keeping the amount of training\\ndata constant. By training over 400 language models ranging from 70 million to\\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\\ncompute-optimal training, the model size and the number of training tokens\\nshould be scaled equally: for every doubling of model size the number of\\ntraining tokens should also be doubled. We test this hypothesis by training a\\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\\nas Gopher but with 70B parameters and 4$\\\\times$ more more data. Chinchilla\\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\\nevaluation tasks. This also means that Chinchilla uses substantially less\\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\\non the MMLU benchmark, greater than a 7% improvement over Gopher.</Sumary><Content>et al., 2021; Fedus et al., 2021; Zoph et al., 2022). The largest dense transformers have passed 500\\nbillion parameters (Smith et al., 2022). The drive to train larger and larger models is clear—so far\\nincreasing the size of language models has been responsible for improving the state-of-the-art in many\\nlanguage modelling tasks. Nonetheless, large language models face several challenges, including\\ntheir overwhelming computational requirements (the cost of training and inference increase with\\nmodel size) (Rae et al., 2021; Thoppilan et al., 2022) and the need for acquiring more high-quality\\ntraining data. In fact, in this work we ﬁnd that larger, high quality datasets will play a key role in any\\nfurther scaling of language models.\\nModelling the scaling behavior.\\nUnderstanding the scaling behaviour of language models and\\ntheir transfer properties has been important in the development of recent large models (Hernandez</Content></Document>\\n<Document index=5 title=Scaling Language Models: Methods, Analysis & Insights from Training Gopher><Sumary>Language modelling provides a step towards intelligent communication systems\\nby harnessing large repositories of written human knowledge to better predict\\nand understand the world. In this paper, we present an analysis of\\nTransformer-based language model performance across a wide range of model\\nscales -- from models with tens of millions of parameters up to a 280 billion\\nparameter model called Gopher. These models are evaluated on 152 diverse tasks,\\nachieving state-of-the-art performance across the majority. Gains from scale\\nare largest in areas such as reading comprehension, fact-checking, and the\\nidentification of toxic language, but logical and mathematical reasoning see\\nless benefit. We provide a holistic analysis of the training dataset and\\nmodel's behaviour, covering the intersection of model scale with bias and\\ntoxicity. Finally we discuss the application of language models to AI safety\\nand the mitigation of downstream harms.</Sumary><Content>Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://aclanthology.org/D\\n19-1244.\\nE. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving. Red\\nteaming language models with language models. To appear, 2022.\\n33\\nScaling Language Models: Methods, Analysis & Insights from Training Gopher\\nA. Peste, E. Ioﬁnova, A. Vladu, and D. Alistarh. AC/DC: alternating compressed/decompressed\\ntraining of deep neural networks. CoRR, abs/2106.12379, 2021. URL https://arxiv.org/ab\\ns/2106.12379.\\nO. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input\\nlength extrapolation. arXiv preprint arXiv:2108.12409, 2021.\\nA. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by\\ngenerative pre-training. 2018.\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised\\nmultitask learners. 2019.</Content></Document>\\n<Document index=6 title=Gemini: A Family of Highly Capable Multimodal Models><Sumary>This report introduces a new family of multimodal models, Gemini, that\\nexhibit remarkable capabilities across image, audio, video, and text\\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\\nsuitable for applications ranging from complex reasoning tasks to on-device\\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\\n32 of these benchmarks - notably being the first model to achieve human-expert\\nperformance on the well-studied exam benchmark MMLU, and improving the state of\\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\\nthat the new capabilities of Gemini models in cross-modal reasoning and\\nlanguage understanding will enable a wide variety of use cases and we discuss\\nour approach toward deploying them responsibly to users.</Sumary><Content>Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth\\nBarnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,\\nIgor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.\\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv\\npreprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\\nXi Chen, Xiao Wang, Soravit Changpinyo, A J Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver,\\nNan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James\\nBradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme,</Content></Document>\\n<Document index=7 title=RWKV: Reinventing RNNs for the Transformer Era><Sumary>Transformers have revolutionized almost all natural language processing (NLP)\\ntasks but suffer from memory and computational complexity that scales\\nquadratically with sequence length. In contrast, recurrent neural networks\\n(RNNs) exhibit linear scaling in memory and computational requirements but\\nstruggle to match the same performance as Transformers due to limitations in\\nparallelization and scalability. We propose a novel model architecture,\\nReceptance Weighted Key Value (RWKV), that combines the efficient\\nparallelizable training of transformers with the efficient inference of RNNs.\\n  Our approach leverages a linear attention mechanism and allows us to\\nformulate the model as either a Transformer or an RNN, thus parallelizing\\ncomputations during training and maintains constant computational and memory\\ncomplexity during inference. We scale our models as large as 14 billion\\nparameters, by far the largest dense RNN ever trained, and find RWKV performs\\non par with similarly sized Transformers, suggesting future work can leverage\\nthis architecture to create more efficient models. This work presents a\\nsignificant step towards reconciling trade-offs between computational\\nefficiency and model performance in sequence processing tasks.</Sumary><Content>lstms. In Proceedings of the 1st Workshop on Repre-\\nsentation Learning for NLP, pages 87–93.\\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile\\nSaulnier, Stas Bekman, M Saiful Bari, Stella Bider-\\nman, Hady Elsahar, Jason Phang, Ofir Press, et al.\\n2022. What language model to train if you have one\\nmillion gpu hours? In Proceedings of BigScience\\nEpisode #5–Workshop on Challenges & Perspectives\\nin Creating Large Language Models.\\nTao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav\\nArtzi. 2018. Simple recurrent units for highly paral-\\nlelizable recurrence. In Proceedings of the 2018 Con-\\nference on Empirical Methods in Natural Language\\nProcessing, pages 4470–4481, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nHanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le.\\n2021. Pay attention to mlps.\\nXuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou,\\nJonathan May, Hao Ma, and Luke Zettlemoyer. 2021.\\nLuna: Linear unified nested attention.\\nAdvances\\nin Neural Information Processing Systems, 34:2441–\\n2453.</Content></Document>\\n<Document index=8 title=BLOOM: A 176B-Parameter Open-Access Multilingual Language Model><Sumary>Large language models (LLMs) have been shown to be able to perform new tasks\\nbased on a few demonstrations or natural language instructions. While these\\ncapabilities have led to widespread adoption, most LLMs are developed by\\nresource-rich organizations and are frequently kept from the public. As a step\\ntowards democratizing this powerful technology, we present BLOOM, a\\n176B-parameter open-access language model designed and built thanks to a\\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\\nWe find that BLOOM achieves competitive performance on a wide variety of\\nbenchmarks, with stronger results after undergoing multitask prompted\\nfinetuning. To facilitate future research and applications using LLMs, we\\npublicly release our models and code under the Responsible AI License.</Sumary><Content>lingual settings. In Challenges & Perspectives in Creating Large Language Models, 2022.\\nURL https://openreview.net/forum?id=rK-7NhfSIW5.\\nYi Tay, Jason Wei, Hyung Won Chung, Vinh Q Tran, David R So, Siamak Shakeri, Xavier\\nGarcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, et al. Transcending\\nscaling laws with 0.1% extra compute. arXiv preprint arXiv:2210.11399, 2022.\\nRyan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar\\nMirkin, and Aaron Gokaslan. Emergent structures and training dynamics in large lan-\\nguage models. In Proceedings of BigScience Episode #5 – Workshop on Challenges &\\nPerspectives in Creating Large Language Models, pages 146–159, virtual+Dublin, May\\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.11.\\nURL https://aclanthology.org/2022.bigscience-1.11.\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Na-\\njoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do</Content></Document>\\n<Document index=9 title=Noisy Channel Language Model Prompting for Few-Shot Text Classification><Sumary>We introduce a noisy channel approach for language model prompting in\\nfew-shot text classification. Instead of computing the likelihood of the label\\ngiven the input (referred as direct models), channel models compute the\\nconditional probability of the input given the label, and are thereby required\\nto explain every word in the input. We use channel models for recently proposed\\nfew-shot learning methods with no or very limited updates to the language model\\nparameters, via either in-context demonstration or prompt tuning. Our\\nexperiments show that, for both methods, channel models significantly\\noutperform their direct counterparts, which we attribute to their stability,\\ni.e., lower variance and higher worst-case accuracy. We also present extensive\\nablations that provide recommendations for when to use channel prompt tuning\\ninstead of other competitive methods (e.g., direct head tuning): channel prompt\\ntuning is preferred when the number of training examples is small, labels in\\nthe training data are imbalanced, or generalization to unseen labels is\\nrequired.</Sumary><Content>models are required to explain every word in the\\ninput, potentially amplifying training signals in the\\nlow data regime. We study the impact of channel\\nmodels for language model prompting where the\\nparameters of the language model are frozen. In\\nparticular, we compare channel models with their\\ndirect counterparts for (1) demonstration methods,\\neither concatenation-based (Brown et al., 2020) or\\nour proposed, ensemble-based (Section 4.1.3), and\\n(2) prompt tuning (Lester et al., 2021).\\nOur experiments on eleven text classiﬁcation\\ndatasets show that channel models outperform their\\ndirect counterparts by a large margin. We attribute\\nthe strong performance of channel models to their\\nstability: they have lower variance and signiﬁ-\\ncantly higher worst-case accuracy then their direct\\ncounterparts over different verbalizers and seeds.\\nWe additionally ﬁnd a direct model with head\\ntuning—tuning the LM head while freezing other\\nparameters—is surprisingly effective, often outper-\\narXiv:2108.04106v3  [cs.CL]  15 Mar 2022</Content></Document>\\n<Document index=10 title=Training Compute-Optimal Large Language Models><Sumary>We investigate the optimal model size and number of tokens for training a\\ntransformer language model under a given compute budget. We find that current\\nlarge language models are significantly undertrained, a consequence of the\\nrecent focus on scaling language models whilst keeping the amount of training\\ndata constant. By training over 400 language models ranging from 70 million to\\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\\ncompute-optimal training, the model size and the number of training tokens\\nshould be scaled equally: for every doubling of model size the number of\\ntraining tokens should also be doubled. We test this hypothesis by training a\\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\\nas Gopher but with 70B parameters and 4$\\\\times$ more more data. Chinchilla\\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\\nevaluation tasks. This also means that Chinchilla uses substantially less\\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\\non the MMLU benchmark, greater than a 7% improvement over Gopher.</Sumary><Content>the number of training tokens should be scaled equally: for every doubling of model size the number\\nof training tokens should also be doubled. We test this hypothesis by training a predicted compute-\\noptimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and\\n4× more more data. Chinchilla uniformly and signiﬁcantly outperforms Gopher (280B), GPT-3 (175B),\\nJurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.\\nThis also means that Chinchilla uses substantially less compute for ﬁne-tuning and inference, greatly\\nfacilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of\\n67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.\\n1. Introduction\\nRecently a series of Large Language Models (LLMs) have been introduced (Brown et al., 2020; Lieber\\net al., 2021; Rae et al., 2021; Smith et al., 2022; Thoppilan et al., 2022), with the largest dense</Content></Document>\\n<Document index=11 title=Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus><Sumary>Large language models have led to remarkable progress on many NLP tasks, and\\nresearchers are turning to ever-larger text corpora to train them. Some of the\\nlargest corpora available are made by scraping significant portions of the\\ninternet, and are frequently introduced with only minimal documentation. In\\nthis work we provide some of the first documentation for the Colossal Clean\\nCrawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set\\nof filters to a single snapshot of Common Crawl. We begin by investigating\\nwhere the data came from, and find a significant amount of text from unexpected\\nsources like patents and US military websites. Then we explore the content of\\nthe text itself, and find machine-generated text (e.g., from machine\\ntranslation systems) and evaluation examples from other benchmark NLP datasets.\\nTo understand the impact of the filters applied to create this dataset, we\\nevaluate the text that was removed, and show that blocklist filtering\\ndisproportionately removes text from and about minority individuals. Finally,\\nwe conclude with some recommendations for how to created and document web-scale\\ndatasets from a scrape of the internet.</Sumary><Content>Florian\\nTramèr,\\nEric\\nWallace,\\nMatthew Jagielski, Ariel Herbert-Voss, Katherine\\nLee, Adam Roberts, Tom Brown, Dawn Song, Ul-\\nfar Erlingsson, Alina Oprea, and Colin Raffel. 2020.\\nExtracting training data from large language models.\\narXiv:2012.07805.\\nIsaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wa-\\nhab, D. V. Esch, Nasanbayar Ulzii-Orshikh, Al-\\nlahsera Tapo, Nishant Subramani, Artem Sokolov,\\nClaytone Sikasote, Monang Setyawan, S. Sarin,\\nSokhar Samb, B. Sagot, C. Rivera, Annette Rios\\nGonzales, Isabel Papadimitriou, S. Osei, Pedro\\nJavier Ortiz Suárez, Iroro Orife, Kelechi Ogueji,\\nRubungo Andre Niyongabo, Toan Q. Nguyen, Math-\\nias Muller, A. Muller, S. Muhammad, N. Muham-\\nmad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov,\\nTapiwanashe Matangira, Colin Leong, Nze Lawson,\\nSneha Kudugunta, Yacine Jernite, M. Jenny, Orhan\\nFirat, Bonaventure F. P. Dossou, Sakhile Dlamini,\\nN. D. Silva, Sakine cCabuk Balli, Stella Rose\\nBiderman, Alessia Battisti, A. Baruwa, Ankur\\nBapna, Pallavi Baljekar, Israel Abebe Azime, Ayo-</Content></Document>\\n<Document index=12 title=REPLUG: Retrieval-Augmented Black-Box Language Models><Sumary>We introduce REPLUG, a retrieval-augmented language modeling framework that\\ntreats the language model (LM) as a black box and augments it with a tuneable\\nretrieval model. Unlike prior retrieval-augmented LMs that train language\\nmodels with special cross attention mechanisms to encode the retrieved text,\\nREPLUG simply prepends retrieved documents to the input for the frozen\\nblack-box LM. This simple design can be easily applied to any existing\\nretrieval and language models. Furthermore, we show that the LM can be used to\\nsupervise the retrieval model, which can then find documents that help the LM\\nmake better predictions. Our experiments demonstrate that REPLUG with the tuned\\nretriever significantly improves the performance of GPT-3 (175B) on language\\nmodeling by 6.3%, as well as the performance of Codex on five-shot MMLU by\\n5.1%.</Sumary><Content>Learning, pp. 7740–7765. PMLR, 2022.\\nGuu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.\\nRetrieval augmented language model pre-training. In\\nInternational Conference on Machine Learning, pp. 3929–\\n3938. PMLR, 2020.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\\nM., Song, D., and Steinhardt, J.\\nMeasuring massive\\nmultitask language understanding. In International Con-\\nference on Learning Representations, 2021. URL https:\\n//openreview.net/forum?id=d7KBjmI3GmQ.\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\\nWelbl, J., Clark, A., et al. Training compute-optimal\\nlarge language models. arXiv preprint arXiv:2203.15556,\\n2022.\\nHu, Y., Hua, H., Yang, Z., Shi, W., Smith, N. A., and Luo, J.\\nPromptcap: Prompt-guided task-aware image captioning.\\narXiv preprint arXiv:2211.09699, 2022.\\nIzacard, G. and Grave, E. Leveraging passage retrieval\\nwith generative models for open domain question an-\\nswering.\\nIn Proceedings of the 16th Conference of</Content></Document>\\n<Document index=13 title=GPT Understands, Too><Sumary>Prompting a pretrained language model with natural language patterns has been\\nproved effective for natural language understanding (NLU). However, our\\npreliminary study reveals that manual discrete prompts often lead to unstable\\nperformance -- e.g., changing a single word in the prompt might result in\\nsubstantial performance drop. We propose a novel method P-Tuning that employs\\ntrainable continuous prompt embeddings in concatenation with discrete prompts.\\nEmpirically, P-Tuning not only stabilizes training by minimizing the gap\\nbetween various discrete prompts, but also improves performance by a sizeable\\nmargin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is\\ngenerally effective for both frozen and tuned language models, under both the\\nfully-supervised and few-shot settings.</Sumary><Content>by mining the training corpus (Jiang et al., 2020b),\\ngradient-based searching (Shin et al., 2020), and us-\\ning pretrained generative models (Gao et al., 2020).\\nHowever, these works aim at searching for better-\\nperforming prompts but do not change the nature\\nof instability for discrete prompts. In addition to\\nthe instability issue, searching in the discrete space\\nmight not be able to fully leverage the gradients\\nfrom backpropagation, which will potentially result\\nin suboptimal solutions. To this end, we explore\\nthe possibility of training continuous prompts to\\nstabilize and improve the performance of language\\nmodel adaptation.\\n2.2\\nP-Tuning\\nFormally, let M be a pretrained language model\\nwith a hidden size of h and a vocabulary size of\\n|V|. Let {(xi, yi))}i be a labeled dataset for an\\nNLU task, where x0:n = {x0, x1, ..., xn} is an\\ninput consisting of a sequence of discrete tokens,\\nand y ∈Y is a label. Our goal is to estimate the\\nconditional probability for classification fM(x) =\\nˆ</Content></Document>\\n<Document index=14 title=Finetuned Language Models Are Zero-Shot Learners><Sumary>This paper explores a simple method for improving the zero-shot learning\\nabilities of language models. We show that instruction tuning -- finetuning\\nlanguage models on a collection of tasks described via instructions --\\nsubstantially improves zero-shot performance on unseen tasks.\\n  We take a 137B parameter pretrained language model and instruction-tune it on\\nover 60 NLP tasks verbalized via natural language instruction templates. We\\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\\ntypes. FLAN substantially improves the performance of its unmodified\\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\\nof finetuning datasets, model scale, and natural language instructions are key\\nto the success of instruction tuning.</Sumary><Content>cessing Systems, volume 33, pp. 1877–1901, 2020. URL https://proceedings.neurips.\\ncc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\nDuo Chai, Wei Wu, Qinghong Han, Fei Wu, and Jiwei Li. Description based text classiﬁcation with\\nreinforcement learning. In Proceedings of the International Conference on Machine Learning, pp.\\n1371–1382. PMLR, 2020. URL http://proceedings.mlr.press/v119/chai20a/\\nchai20a.pdf.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards,\\nYura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained\\non code. arXiv preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.\\n03374.\\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\\nZettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing, pp. 2174–2184, 2018. URL https:\\n//aclanthology.org/D18-1241.</Content></Document>\\n<Document index=15 title=Scaling Language Models: Methods, Analysis & Insights from Training Gopher><Sumary>Language modelling provides a step towards intelligent communication systems\\nby harnessing large repositories of written human knowledge to better predict\\nand understand the world. In this paper, we present an analysis of\\nTransformer-based language model performance across a wide range of model\\nscales -- from models with tens of millions of parameters up to a 280 billion\\nparameter model called Gopher. These models are evaluated on 152 diverse tasks,\\nachieving state-of-the-art performance across the majority. Gains from scale\\nare largest in areas such as reading comprehension, fact-checking, and the\\nidentification of toxic language, but logical and mathematical reasoning see\\nless benefit. We provide a holistic analysis of the training dataset and\\nmodel's behaviour, covering the intersection of model scale with bias and\\ntoxicity. Finally we discuss the application of language models to AI safety\\nand the mitigation of downstream harms.</Sumary><Content>32\\nScaling Language Models: Methods, Analysis & Insights from Training Gopher\\nP. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston,\\nO. Kuchaiev, G. Venkatesh, and H. Wu. Mixed precision training. In International Conference on\\nLearning Representations, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\nT. Mikolov, M. Karaﬁát, L. Burget, J. Cernock`\\ny, and S. Khudanpur. Recurrent neural network based\\nlanguage model. In Interspeech, volume 2, pages 1045–1048. Makuhari, 2010.\\nT. Mikolov, A. Deoras, S. Kombrink, L. Burget, and J. H. Černocký. Empirical evaluation and combina-\\ntion of advanced language modeling techniques. In Interspeech, 2011.\\nT. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word representations in vector\\nspace. arXiv preprint arXiv:1301.3781, 2013.\\nA. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh, C. Yu, and P. Micikevicius.\\nAccelerating sparse deep neural networks, 2021.</Content></Document>\\n    </context>\\n\\nLembre-se de que o objetivo de cada resumo é servir como um guia de estudo para um Cientista de Dados especialista em AI, Estatística e Deep Learning, com conhecimentos avançados em tecnologia e programação.\\n\\n!!! Expressões matemáticas usando $ ao invés de \\\\( e \\\\), e $$ ao invés de \\\\[ e \\\\] !!!\\n!!! E quando citar variáveis, funções ou trechos de expressões matemáticas use $f(x)$ ao invés de **f(x)** ou \\\\[ e \\\\( !!!\\n\\n!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!\\n\\n<template>\\n\\nCrie um resumo avançado, aprofundado e elaborado sobre X (mínimo de 8 páginas, extenso, não poupe detalhes, aprofunde-se em conceitos técnicos e matemáticos)\\n\\n**X =** \\n\\nUtilize a formatação abaixo como inspiração para o resumo, mas faça as adaptações necessárias com o objetivo de criar o melhor resumo possível. Lembre-se de que o objetivo é servir como um guia de estudo para um Cientista de Dados especialista em AI, Estatística e Deep Learning, com conhecimentos avançados em tecnologia e programação.\\n\\nOrientações para escrever o resumo:\\n\\n**Organização e Estrutura**: Garanta que cada seção do resumo esteja bem organizada e siga uma lógica clara. Utilize títulos e subtítulos para facilitar a navegação. Crie uma estrutura hierárquica coerente, com uma introdução, desenvolvimento e conclusão bem definidos.\\n\\n**Detalhamento**: Aprofunde-se nos conceitos técnicos e matemáticos, fornecendo explicações detalhadas, exemplos práticos e demonstrações passo a passo quando necessário.\\n\\n**Destaques**: Sempre que mencionar os conceitos principais no texto, utilize **negrito** para destacá-los. Quando quiser inserir uma citação importante ou parafrasear alguém, utilize *itálico*. Utilize caixas de destaque, como notas, avisos e dicas, para enfatizar informações cruciais.\\n\\n**Estilo e tom:** Escreva de forma acadêmica e formal, mas use emojis quando necessário para dar destaque a alguma informação, por exemplo, ao destacar um tópico usando blockquotes. Utilize emojis como ⚠️❗✔️💡 e outros que façam sentido dado o conteúdo. Mantenha um tom instrutivo e explicativo ao longo do texto.\\n\\nTemplate para o resumo:\\n\\n## Título do Resumo (seja breve)\\n\\nInicie com uma introdução concisa, porém abrangente, que contextualize a importância do tema.\\n\\n### Principais Conceitos\\n\\n| Conceito       | Explicação                                                   |\\n| -------------- | ------------------------------------------------------------ |\\n| **Conceito 1** | Forneça uma explicação concisa do conceito, explorando as bases teóricas e suas aplicações práticas. |\\n| **Conceito 2** | Forneça uma explicação concisa do conceito, explorando as bases teóricas e suas aplicações práticas. |\\n\\nUtilize as formatações abaixo como exemplo para destacar informações importantes e críticas:\\n\\n> ⚠️ **Nota Importante**: Use esta formatação para destacar informações críticas ou observações que não podem ser ignoradas, assegurando que se destaquem no contexto do resumo.\\n\\n> ❗ **Ponto de Atenção**: Use esta formatação para destacar informações críticas ou observações que requerem maior atenção ao implementar, pois colocam em risco o uso correto do conceito e devem ser levadas em conta pelo usuário.\\n\\n> ✔️ **Ponto de Destaque** (técnicos e teóricos): Use esta formatação para destacar informações críticas ou observações teóricas ou técnicas que impactam de forma positiva na compreensão do fenômeno, como resultados importantes que não podem ser ignorados.\\n\\n### Abstract\\n\\nCopie o abstract ou sumario consolidado dos documentos usados no resumo nessa de forma que o leitor tenha essa referência em mãos quando for estudar o artigo.\\n\\n### [Explicação de algum tópico ou conceito]\\n\\nElabore de forma aprofundada sobre os tópicos e conceitos do tema X, de modo que o resumo seja avançado, detalhado, bem escrito e cumpra os objetivos do texto. Não poupe detalhes!\\n\\nQuando for contrastar, comparar, etc., informações, use a formatação de lista de tópicos como no exemplo:\\n\\n#### 👍Vantagens\\n\\n* Vantagem 1: explicação detalhada e concisa do ponto de vantagem (exemplo)\\n* Vantagem 2: explicação detalhada e concisa do ponto de vantagem (exemplo)\\n\\n#### 👎Desvantagens\\n\\n* Desvantagem 1: explicação detalhada e concisa do ponto de desvantagem (exemplo)\\n* Desvantagem 2: explicação detalhada e concisa do ponto de desvantagem (exemplo)\\n\\nOu de tabela, dependendo de qual melhor se ajustar ao conteúdo:\\n\\n| 👍 Vantagens                                                  | 👎 Desvantagens                                               |\\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\\n| Explicação detalhada e concisa do ponto de vantagem (exemplo) | Explicação detalhada e concisa do ponto de desvantagem (exemplo) |\\n| Explicação detalhada e concisa do ponto de vantagem (exemplo) | Explicação detalhada e concisa do ponto de desvantagem (exemplo) |\\n\\nUse esse exemplo apenas como inspiração e utilize esses tipos de formatação de acordo com a necessidade de elaborar sobre algum ponto tópico do tema.\\n\\n### [Explicação de algum tópico ou conceito teórico]\\n\\nApresente definições matemáticas e teóricas detalhadas, sem economizar em complexidade. Use a seguinte formatação para equações importantes, garantindo que sejam facilmente legíveis e centralizadas. Por exemplo:\\n\\nO Teorema de Bayes é um resultado fundamental na teoria da probabilidade que descreve como atualizar as probabilidades de uma hipótese com base em novas evidências. Ele estabelece uma relação entre as probabilidades condicionais de dois eventos.\\n\\nSeja $A$ e $B$ dois eventos, o Teorema de Bayes afirma que:\\n\\n$$\\nP(A|B) = \\\\frac{P(B|A)P(A)}{P(B)}\\n$$\\n\\nonde:\\n\\n- $P(A|B)$ é a probabilidade condicional de $A$ dado $B$, também conhecida como probabilidade a posteriori.\\n- $P(B|A)$ é a probabilidade condicional de $B$ dado $A$, também conhecida como verossimilhança.\\n- $P(A)$ é a probabilidade a priori de $A$.\\n- $P(B)$ é a probabilidade marginal de $B$, que atua como uma constante normalizadora.\\n\\nA probabilidade marginal $P(B)$ pode ser calculada usando a lei da probabilidade total:\\n\\n$$\\nP(B) = P(B|A)P(A) + P(B|\\\\neg A)P(\\\\neg A)\\n$$\\n\\nonde $\\\\neg A$ denota o evento complementar de $A$.\\n\\nProssiga com uma discussão detalhada para explicar o funcionamento da equação e suas implicações do conceito. Faça uma análise de seu comportamento matemático, se possível. Por exemplo:\\n\\nO Teorema de Bayes permite atualizar nossas crenças (probabilidades) sobre uma hipótese $A$ após observar novas evidências $B$. Ele combina nossa crença prévia em $A$ (probabilidade a priori) com a probabilidade de observar $B$ dado que $A$ é verdadeiro (verossimilhança) para obter nossa crença atualizada em $A$ dado $B$ (probabilidade a posteriori).\\n\\n> ✔️ **Ponto de Destaque**: O Teorema de Bayes fornece uma estrutura matemática para o raciocínio probabilístico e a atualização de crenças com base em novas informações. Ele é amplamente aplicado em áreas como aprendizado de máquina, estatística, ciência da computação e tomada de decisão.\\n\\n!!! Expressões matemáticas usando $ ao invés de \\\\( e \\\\), e $$ ao invés de \\\\[ e \\\\] !!!\\n!!! E quando citar variáveis, funções ou trechos de expressões matemáticas use $f(x)$ ao invés de **f(x)** ou \\\\[ e \\\\( !!!\\n\\n### [Explicação de algum tópico ou conceito técnico]\\n\\nColoque aqui informações relevantes e concisas para explicar a aplicação do tópico e como implementá-lo. Quando houver necessidade de mostrar um código na linguagem apropriada, use a formatação:\\n\\n```python\\nimport lib # assuma que as dependências já estão instaladas\\n\\n# Comentário para elucidar apenas aspectos importantes\\ndef minha_funcao(param):\\n\\treturn lib.outra_funcao(param)\\n```\\n\\nMantenha os snippets claros, concisos e o menor possível, com foco na funcionalidade principal. Não adicione códigos de setup como pip install, downloads, etc.\\n\\n!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!\\n\\n### [Aplicações|Trabalhos futuros|Extensões|etc]\\n\\nSe houver necessidade de falar sobre aplicações do conceito, trabalhos e pesquisas futuras, áreas de interesse e extensões do conceito, use o seguinte formato:\\n\\n| Conceito       | Explicação                                                   |\\n| -------------- | ------------------------------------------------------------ |\\n| **Conceito 1** | Explicação detalhada do conceito, incluindo exemplos práticos e aplicações. |\\n| **Conceito 2** | Explicação detalhada do conceito, incluindo exemplos práticos e aplicações. |\\n\\n### [Tópicos Relacionados]\\n\\nPara orientar o usuário desse guia, crie uma lista de próximos tópicos avançados relacionados, quando houver necessidade:\\n\\n- [ ] Tópico relacionado 1\\n- [ ] Tópico relacionado 2\\n- [ ] etc.\\n\\n### Conclusão\\n\\nResuma todos os tópicos apresentados em uma conclusão sucinta e objetiva.\\n\\n### Referências\\n\\nAdicione aqui as referências da seguinte forma:\\n\\n[1] Attention is All You Need\\n[2] Other paper name\\n[3] Etc\\n\\n!!! Lembre-se de que esse template é apenas um guia e você deve apenas se inspirar nele, sem a necessidade de replicar a mesma estrutura ao pé da letra. Foque no objetivo !!!\\n\\n!!! NÃO POUPE DETALHES, SEJA O MAIS APROFUNDADO POSSÍVEL !!!\\n\\n!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!\\n\\n!!! Expressões matemáticas usando $ ao invés de \\\\( e \\\\), e $$ ao invés de \\\\[ e \\\\] !!!\\n!!! E quando citar variáveis, funções ou trechos de expressões matemáticas use $f(x)$ ao invés de **f(x)** ou \\\\[ e \\\\( !!!\\n\\n</template>\\n\\nDiretrizes para o resumo:\\nOs resumos devem ser avançados;\\nOs resumos devem ser baseados nos principais aspectos do conceito abordado no texto, como técnicas ou funcionalidades específicas demonstradas em cada subcapítulo;\\nO resumo deve conter todas principais informações presentes no texto sem omitir nenhum dado importante, com foco especial em não pular nenhum conceitos, resultados importante, argumentos, etc;\\nO resumo deve conter as equações apresentadas, tabelas e outras informações críticas para um entendimento aprofundando e avançado do conteúdo;\\nO resumo deve ser escrito de uma maneira acadêmica, do not repeat text.\\nVocê deve usar o <context> da melhor maneira possível para responder a query do usuário e escrever o resumo segundo as diretrizes;\\nYou must not tell the user to open any link or visit any website to get the answer. You must provide the answer in the response itself;\\nVocê não deve pedir para o usuário abrir um link ou visitar um site para ver a resposta. Você deve responder você mesmo;\\nYou have to cite the answer using [number] notation. The number is the idx on the documents. You must cite the sentences with their relevent context number. You must cite each and every part of the answer so the user can know where the information is coming from.\\nPlace these citations at the end of that particular sentence. You can cite the same sentence multiple times if it is relevant to the user's query like [number1][number2].\\nHowever you do not need to cite it using the same number. You can use different numbers to cite the same sentence multiple times. The number refers to the number of the search result (passed in the context) used to generate that part of the answer.\\nColoque os resultados um texto coerente ao invés de apenas listar em tópicos, também foque em usar as formatações mostradas no template.\\n\\n!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!\\n\\n\\nX = Stability of training Large Language Models\\n\\nResposta em português:\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}